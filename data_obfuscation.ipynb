{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "vsiKgYkf4U8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://spacy.io/usage/spacy-101"
      ],
      "metadata": {
        "id": "3Ye7X76zPwA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "zslVYvMJPo8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "metadata": {
        "id": "ra_AiKj6P-BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def mask_entities(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            # PERSON: People, including fictional characters\n",
        "            text = text.replace(ent.text, \"[PERSON-REDACTED]\")\n",
        "        elif ent.label_ == \"NORP\":\n",
        "            # NORP: Nationalities or religious or political groups\n",
        "            text = text.replace(ent.text, \"[NORP-REDACTED]\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "X0vI2QzuQFuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask_entities(\"My name is John Abraham and I am an Indian\"))"
      ],
      "metadata": {
        "id": "f2gIxrFEQRiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask_entities(\"Noah's Ark is a large ship mentioned in the Book of Genesis in the Bible. According to the story, God told Noah to build the ark to save himself, his family, and a pair of every kind of animal from a great flood that would destroy the rest of the world. The ark was built to be 300 cubits long, 50 cubits wide, and 30 cubits high (approximately 134 meters long, 22 meters wide, and 13 meters high). It had three decks and was made of gopher wood.\"))"
      ],
      "metadata": {
        "id": "5ja-sDTYQRTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask_entities(\"Chandrayaan-3 is the third Indian lunar exploration mission under the Indian Space Research Organisation's Chandrayaan programme. It was launched on July 14, 2023, and successfully soft-landed on the moon on August 23, 2023. \"))"
      ],
      "metadata": {
        "id": "8EXOjggdQRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SciSpacy"
      ],
      "metadata": {
        "id": "iiMC5HwWMefT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://spacy.io/universe/project/scispacy"
      ],
      "metadata": {
        "id": "gEINeOrmMwus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-requisites"
      ],
      "metadata": {
        "id": "axPwXgI6CjWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz"
      ],
      "metadata": {
        "id": "sSidlb5rQjfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load & run"
      ],
      "metadata": {
        "id": "Xm53K7XnCnza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import scispacy\n",
        "\n",
        "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
        "\n",
        "def redact_drug_names(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        print(ent, ent.label_)\n",
        "        if ent.label_ == \"CHEMICAL\":\n",
        "            text = text.replace(ent.text, \"[REDACTED]\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "jW-116iOQrVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The patient was prescribed aspirin and ibuprofen for their headache.\"\n",
        "redacted_text = redact_drug_names(text)\n",
        "print(redacted_text)"
      ],
      "metadata": {
        "id": "10lTaVAHQz9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stanza"
      ],
      "metadata": {
        "id": "UxCUM2BAO_pF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stanfordnlp.github.io/stanza/"
      ],
      "metadata": {
        "id": "IbMNfIdMPVUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-requisites"
      ],
      "metadata": {
        "id": "HArIoMg7NKsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanza"
      ],
      "metadata": {
        "id": "7Qt7FpPXR1gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the model"
      ],
      "metadata": {
        "id": "2PQPmsAYNOPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('en') # download English model"
      ],
      "metadata": {
        "id": "XhaNPW0cR7KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intializing the neural pipeline"
      ],
      "metadata": {
        "id": "6_w-sOjBNYfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
        "doc = nlp(\"Narendra Damodardas Modi is an Indian politician.\") # run annotation over a sentence"
      ],
      "metadata": {
        "id": "SLJtbHLQR_8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc.entities)"
      ],
      "metadata": {
        "id": "5N7P7h6rSHLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Presidio"
      ],
      "metadata": {
        "id": "T1XGZNn0SUbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/microsoft/presidio"
      ],
      "metadata": {
        "id": "5C8iOSx_WZAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON Text Anonymization"
      ],
      "metadata": {
        "id": "LEuwq2q1h_AQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install"
      ],
      "metadata": {
        "id": "3DoOUjG0Ys0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download presidio\n",
        "!pip install presidio_analyzer presidio_anonymizer\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "CwEAeVdeWxDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Imports"
      ],
      "metadata": {
        "id": "38gNuy4HY7zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Dict, Union, Iterator, Iterable\n",
        "import collections\n",
        "from dataclasses import dataclass\n",
        "import pprint\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult\n",
        "from presidio_anonymizer import AnonymizerEngine, BatchAnonymizerEngine\n",
        "from presidio_anonymizer.entities import EngineResult"
      ],
      "metadata": {
        "id": "CVusUWPYY9Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = AnalyzerEngine()\n",
        "batch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer)\n",
        "batch_anonymizer = BatchAnonymizerEngine()"
      ],
      "metadata": {
        "id": "nyX-VV55eh9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nested_dict = {\n",
        "    \"key_a\": {\"key_a1\": \"My phone number is 212-121-1424\"},\n",
        "    \"key_b\": {\"www.abc.com\"},\n",
        "    \"key_c\": 3,\n",
        "    \"names\": [\"James Bond\", \"Clark Kent\", \"Hakeem Olajuwon\", \"No name here!\"]\n",
        "}\n"
      ],
      "metadata": {
        "id": "SP9ZuKQSdUzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze dict\n",
        "analyzer_results = batch_analyzer.analyze_dict(input_dict = nested_dict, language=\"en\")\n",
        "\n",
        "# Anonymize dict\n",
        "anonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results = analyzer_results)\n",
        "pprint.pprint(anonymizer_results)"
      ],
      "metadata": {
        "id": "iaPN8zwzdnrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCIOM Image Anonymization"
      ],
      "metadata": {
        "id": "4l5sOmqXiHaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install"
      ],
      "metadata": {
        "id": "OBWD2s7Uiqba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! apt install tesseract-ocr\n",
        "! apt install libtesseract-dev"
      ],
      "metadata": {
        "id": "8J4S7AQdieay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Pillow\n",
        "! pip install pytesseract"
      ],
      "metadata": {
        "id": "TGmS_vT7iuPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "!pip install presidio_image_redactor\n",
        "\n",
        "# Presidio image redactor uses the presidio-analyzer\n",
        "# which requires a spaCy language model:\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "lfbi3wISiwlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import pydicom\n",
        "from presidio_image_redactor import DicomImageRedactorEngine"
      ],
      "metadata": {
        "id": "V3yWDIofjJY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_dicom_images(\n",
        "    instance_original: pydicom.dataset.FileDataset,\n",
        "    instance_redacted: pydicom.dataset.FileDataset,\n",
        "    figsize: tuple = (11, 11)\n",
        ") -> None:\n",
        "    \"\"\"Display the DICOM pixel arrays of both original and redacted as images.\n",
        "\n",
        "    Args:\n",
        "        instance_original (pydicom.dataset.FileDataset): A single DICOM instance (with text PHI).\n",
        "        instance_redacted (pydicom.dataset.FileDataset): A single DICOM instance (redacted PHI).\n",
        "        figsize (tuple): Figure size in inches (width, height).\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(1, 2, figsize=figsize)\n",
        "    ax[0].imshow(instance_original.pixel_array, cmap=\"gray\")\n",
        "    ax[0].set_title('Original')\n",
        "    ax[1].imshow(instance_redacted.pixel_array, cmap=\"gray\")\n",
        "    ax[1].set_title('Redacted')"
      ],
      "metadata": {
        "id": "gp2jhVGFjPOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = DicomImageRedactorEngine()"
      ],
      "metadata": {
        "id": "3V7kPDKFjTJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single DICOM (.dcm) file or directory containing DICOM files\n",
        "input_path = 'sample_data/input'\n",
        "\n",
        "# Directory where the output will be written\n",
        "output_parent_dir = 'sample_data/output'"
      ],
      "metadata": {
        "id": "9NPj1pq9mOqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redact text PHI from DICOM images\n",
        "engine.redact_from_directory(\n",
        "    input_dicom_path = input_path,\n",
        "    output_dir = output_parent_dir,\n",
        "    fill=\"contrast\"\n",
        ")"
      ],
      "metadata": {
        "id": "178IuUOWmTGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original DICOM images\n",
        "p = Path(input_path).glob(\"**/*.dcm\")\n",
        "original_files = [x for x in p if x.is_file()]\n",
        "\n",
        "# Redacted DICOM images\n",
        "p = Path(output_parent_dir).glob(\"**/*.dcm\")\n",
        "redacted_files = [x for x in p if x.is_file()]"
      ],
      "metadata": {
        "id": "xfuN1tRImXBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(original_files)):\n",
        "    original_file = pydicom.dcmread(original_files[i])\n",
        "    redacted_file = pydicom.dcmread(redacted_files[i])\n",
        "\n",
        "    compare_dicom_images(original_file, redacted_file)"
      ],
      "metadata": {
        "id": "fAY5qoMWmbVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Anonymization"
      ],
      "metadata": {
        "id": "DH3HYLtYYE45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prerequisites"
      ],
      "metadata": {
        "id": "Up68CwKHjVdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install pdfminer.six\n",
        "!pip install pikepdf"
      ],
      "metadata": {
        "id": "dKHvIGA4ZV09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Presidio\n",
        "from presidio_analyzer import AnalyzerEngine, PatternRecognizer\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig\n",
        "\n",
        "# For console output\n",
        "from pprint import pprint\n",
        "\n",
        "# For extracting text\n",
        "from pdfminer.high_level import extract_text, extract_pages\n",
        "from pdfminer.layout import LTTextContainer, LTChar, LTTextLine\n",
        "\n",
        "# For updating the PDF\n",
        "from pikepdf import Pdf, AttachedFileSpec, Name, Dictionary, Array"
      ],
      "metadata": {
        "id": "uEFFyCAPbTwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze the text in the PDF"
      ],
      "metadata": {
        "id": "w1iCY5X8jY1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "analyzed_character_sets = []\n",
        "\n",
        "for page_layout in extract_pages(\"./sample_data/sample.pdf\"):\n",
        "    for text_container in page_layout:\n",
        "        if isinstance(text_container, LTTextContainer):\n",
        "\n",
        "            # The element is a LTTextContainer, containing a paragraph of text.\n",
        "            text_to_anonymize = text_container.get_text()\n",
        "\n",
        "            # Analyze the text using the analyzer engine\n",
        "            analyzer_results = analyzer.analyze(text=text_to_anonymize, language='en')\n",
        "\n",
        "            if text_to_anonymize.isspace() == False:\n",
        "                print(text_to_anonymize)\n",
        "                print(analyzer_results)\n",
        "\n",
        "            characters = list([])\n",
        "\n",
        "            # Grab the characters from the PDF\n",
        "            for text_line in filter(lambda t: isinstance(t, LTTextLine), text_container):\n",
        "                    for character in filter(lambda t: isinstance(t, LTChar), text_line):\n",
        "                            characters.append(character)\n",
        "\n",
        "\n",
        "            # Slice out the characters that match the analyzer results.\n",
        "            for result in analyzer_results:\n",
        "                start = result.start\n",
        "                end = result.end\n",
        "                analyzed_character_sets.append({\"characters\": characters[start:end], \"result\": result})"
      ],
      "metadata": {
        "id": "DFwFgYQ8bnGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create phrase bounding boxes"
      ],
      "metadata": {
        "id": "8YzwYGY9jv6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the bounding boxes into a single bounding box.\n",
        "def combine_rect(rectA, rectB):\n",
        "    a, b = rectA, rectB\n",
        "    startX = min( a[0], b[0] )\n",
        "    startY = min( a[1], b[1] )\n",
        "    endX = max( a[2], b[2] )\n",
        "    endY = max( a[3], b[3] )\n",
        "    return (startX, startY, endX, endY)\n",
        "\n",
        "analyzed_bounding_boxes = []\n",
        "\n",
        "# For each character set, combine the bounding boxes into a single bounding box.\n",
        "for analyzed_character_set in analyzed_character_sets:\n",
        "    completeBoundingBox = analyzed_character_set[\"characters\"][0].bbox\n",
        "\n",
        "    for character in analyzed_character_set[\"characters\"]:\n",
        "        completeBoundingBox = combine_rect(completeBoundingBox, character.bbox)\n",
        "\n",
        "    analyzed_bounding_boxes.append({\"boundingBox\": completeBoundingBox, \"result\": analyzed_character_set[\"result\"]})"
      ],
      "metadata": {
        "id": "iSQnR16BebWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add highlight annotations"
      ],
      "metadata": {
        "id": "uBm6fA5tj61W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = Pdf.open(\"./sample_data/sample.pdf\")\n",
        "\n",
        "annotations = []\n",
        "\n",
        "# Create a highlight annotation for each bounding box.\n",
        "for analyzed_bounding_box in analyzed_bounding_boxes:\n",
        "\n",
        "    boundingBox = analyzed_bounding_box[\"boundingBox\"]\n",
        "\n",
        "    # Create the annotation.\n",
        "    # We could also create a redaction annotation if the ongoing workflows supports them.\n",
        "    highlight = Dictionary(\n",
        "        Type=Name.Annot,\n",
        "        Subtype=Name.Highlight,\n",
        "        QuadPoints=[boundingBox[0], boundingBox[3],\n",
        "                    boundingBox[2], boundingBox[3],\n",
        "                    boundingBox[0], boundingBox[1],\n",
        "                    boundingBox[2], boundingBox[1]],\n",
        "        Rect=[boundingBox[0], boundingBox[1], boundingBox[2], boundingBox[3]],\n",
        "        C=[1, 0, 0],\n",
        "        CA=0.5,\n",
        "        T=analyzed_bounding_box[\"result\"].entity_type,\n",
        "    )\n",
        "\n",
        "    annotations.append(highlight)\n",
        "\n",
        "# Add the annotations to the PDF.\n",
        "pdf.pages[0].Annots = pdf.make_indirect(annotations)\n",
        "\n",
        "# And save.\n",
        "pdf.save(\"./sample_data/sample_annotated.pdf\")"
      ],
      "metadata": {
        "id": "ZHPjtA9Sf1V_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}